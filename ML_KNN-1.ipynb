{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847adbb7-db76-4977-981e-bb4bb638709c",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd065e-f3c9-4607-a19d-30f13d51dd2b",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It works on the principle that data points with similar features tend to have similar labels or target values. Given a new data point, KNN finds the \"K\" closest data points from the training set and uses their labels (for classification) or average/target values (for regression) to predict the label or value of the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768ab2e-88b4-43b3-a1dd-6c72edbb4923",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491180f2-5135-481c-9aa5-c7f7f6e92a5f",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8edb9-4e1e-477c-a364-b94ab71036a4",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Selecting the value of K in KNN is a critical decision. A small K value may lead to noisy predictions, while a large K value may oversmooth the decision boundaries. One common approach is to use cross-validation to evaluate the performance of different K values on a validation set. The value of K that gives the best performance (e.g., highest accuracy for classification or lowest error for regression) on the validation set can be chosen as the final K value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7fe27-1fe2-442b-9990-ddf2b4958328",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acec684-dfcf-48b9-919d-91939ae4fe94",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d565c-385c-439b-9285-61ed94caf9fa",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "The main difference lies in their output types:\n",
    "\n",
    ">KNN Classifier: It is used for classification tasks where the output is a categorical label. The KNN classifier assigns the class label to the new data point based on the majority class among its K nearest neighbors.\n",
    "\n",
    ">KNN Regressor: It is used for regression tasks where the output is a continuous value. The KNN regressor predicts the output value for the new data point by taking the average of the target values of its K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68aa9b-06e7-48ba-9454-ab662e3c7fc2",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d8454-0f91-4c58-9047-9330eaa2c9b0",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc33b3-63e6-4865-85f7-d4d8babbb1e0",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    " The performance of the KNN algorithm can be measured using various evaluation metrics depending on the task:\n",
    "\n",
    ">For classification, common metrics include accuracy, precision, recall, F1-score, and area under the Receiver Operating Characteristic (ROC) curve.\n",
    "\n",
    ">For regression, common metrics include Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbaf43-6ff9-49ad-b189-2a56c6f188e8",
   "metadata": {},
   "source": [
    "                      -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dbde1-120b-428e-9820-73b17987c381",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6469c-811c-4261-854e-d172b6171204",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    " The curse of dimensionality refers to a problem in KNN and other distance-based algorithms, where the performance degrades as the number of features (dimensions) in the dataset increases. In high-dimensional spaces, data points become sparse, and the notion of distance between points loses its significance. As a result, the KNN algorithm may become less effective and computationally expensive in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd56842-96dd-463e-9a53-abe5f7acc2c6",
   "metadata": {},
   "source": [
    "                       -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfcd8a-e250-42a7-8424-c3a5f8fe7a69",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ec4f9-dded-413f-b97e-d4ac1759a65a",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    " Handling missing values in KNN requires imputing the missing values before applying the algorithm. Some common strategies include:\n",
    "\n",
    ">Simple imputation: Replacing missing values with the mean, median, or mode of the feature.\n",
    "\n",
    ">KNN-based imputation: Using KNN to find the K nearest neighbors of the data point with missing values and imputing the missing values with the average or weighted average of the neighbors' values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195c9bd-aba6-49ad-a195-6b541eb54fb2",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f5010-8426-42d8-88ce-318675d1395c",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455a521-5fe1-4deb-921a-b6842cf84a55",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    " The performance of the KNN classifier and regressor depends on the nature of the problem:\n",
    "\n",
    ">KNN Classifier: It works well when the decision boundaries are relatively simple and classes are well-separated. It is suitable for problems with discrete categorical output, such as image recognition, text categorization, or sentiment analysis.\n",
    "\n",
    ">KNN Regressor: It performs well when the relationship between features and target values is continuous and not easily separable. It is suitable for problems like predicting house prices, stock prices, or any other continuous numerical output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3b616-2f74-4a73-88b6-1f04edafdd88",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06e381-01a3-4462-922f-7719d66e0324",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b4e11b-f364-4a79-b524-170c89ab43e4",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    " Strengths:\n",
    "\n",
    ">Simple and easy to understand.\n",
    "\n",
    ">No training phase, which makes it computationally efficient during inference.\n",
    "\n",
    ">Works well with nonlinear relationships.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    ">Sensitive to the choice of K.\n",
    "\n",
    ">Computationally expensive during inference for large datasets.\n",
    "\n",
    ">Prone to the curse of dimensionality in high-dimensional spaces.\n",
    "\n",
    "To address the sensitivity to K, cross-validation can be used to find the optimal K value. To tackle computational inefficiency, approximate algorithms like KD-trees or Ball-trees can be employed. The curse of dimensionality can be mitigated by using dimensionality reduction techniques or feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf10c1f-245e-40f7-bc2c-e086a92eea91",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599fcf3-4f03-4f97-8faa-89201bd76dac",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52909f43-97bb-45f6-ad67-c2a26fc7d6de",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    " Euclidean distance and Manhattan distance are two common distance metrics used in KNN to measure the similarity between data points.\n",
    "\n",
    "Euclidean distance: It is the straight-line distance between two points in Euclidean space. In 2-dimensional space, it is the length of the line connecting two points. Mathematically, for two points (x1, y1) and (x2, y2), the Euclidean distance is calculated as âˆš((x2-x1)^2 + (y2-y1)^2).\n",
    "\n",
    "Manhattan distance: It is the distance between two points measured along the axes at right angles. In 2-dimensional space, it is the sum of the absolute differences between the x-coordinates and y-coordinates of two points. Mathematically, for two points (x1, y1) and (x2, y2), the Manhattan distance is calculated as |x2-x1| + |y2-y1|."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f3d9c-e193-4f33-bc16-1497481e05a2",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b51f1-1489-47ad-8bad-d59e3b9aa53c",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418235fc-d8d5-45de-87d0-e36fb0e6b75e",
   "metadata": {},
   "source": [
    "#Answer\n",
    "\n",
    "Feature scaling plays an essential role in KNN because the algorithm relies on the distance between data points. When the features have different scales, those with larger magnitudes can dominate the distance calculation and overshadow features with smaller magnitudes. This can lead to biased results.\n",
    "\n",
    "By performing feature scaling (e.g., normalization or standardization), all features are brought to a similar scale, ensuring that no single feature dominates the distance calculation. This helps KNN to make better and more reliable predictions by taking into account the contribution of all features equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e27a355-3583-4a73-87a7-10da1be0807d",
   "metadata": {},
   "source": [
    "                        -------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
